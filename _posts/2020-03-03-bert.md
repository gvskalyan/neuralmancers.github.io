---
title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
categories: NLP
tags: [attention, pre-training, transformers, nlu]
---
## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019)
> Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova

## Highlights

* Prior to BERT, all language model pre-training techniques such as [Open AI GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) relied only on uni-directional LMs.
* Bidirectionality will be of help while learning representations that can be used in downstream tasks such as question answering, etc.
* Only one or few additional output layers along with fine-tuning gets state-of-the-art results in multiple downstream tasks.
* No task specific architecture modifications are done.

## Feature based vs Fine-tuning based pre-trained representations

* Models such as [ElMo](https://allennlp.org/elmo) use the pre-trained representations as additional features over a task specific architecture.
* GPT introduces only a minimal set of task-specific params and is trained on the downstream tasks along with fine-tuning the pre-trained params.
* All these models have only been leveraging uni-directional LMs.

## BERT objectives

### MLM - Masked Language Model

* Randomly mask some of the input tokens.
* Predict the original vocab ID of the masked token based only on its content.
* Next word prediction (which is the usual LM objective) is trivial in a bi-directional setting. Hence this objective makes sense.
* 15 % random tokens are masked.

### NSP - Next Sentence Prediction

* Jointly pre-train text representations with this objective.
* Next sentence prediction with sentence pairs taken from a monolingual corpus.
* 50 % of correct pairs and 50 % sampled at random.

## Pre-training & Fine-tuning

* Pre-training: Model is trained on unlabelled data
* Fine-tuning: Initialize with pre-trained params and fine-tune with lablled data for downstream tasks.

## Size

* Bert_base: L (num layers) = 12, H (hidden size) = 768, A (attention heads) = 12, params = 110M
* Bert_large: L = 24, H = 1024, A = 16, params = 340M

## Input/Output representations

* Representations should be in such a way as to support multiple downstream tasks.
* Represent both single/pair of sentences in a single example sequence.

### Token Embeddings

* [CLS] Sentence 1 [SEP] Sentence 2 [SEP]
* Each sentence is tokenized using [wordpiece embeddings](https://github.com/google/sentencepiece)

### Segment Embeddings

* Additional embeddings are learnt to identify whether a token belongs to the first or second sentence.

### Position Embeddings

* These embeddings are learnt to make the representations be aware of the relative positions of the tokens in the sequence. Max length here is 512.

All the above embeddings are concatenated together to form the input representation.

To be continued..

